{
  "execution_plan": {
    "phase_id": "SQL Pipeline & dbt Support",
    "waves": [
      {
        "wave_id": 1,
        "strategy": "PARALLEL_SWARM",
        "rationale": "Wave 1: 17 independent task(s) with no file conflicts",
        "tasks": [
          {
            "task_id": "T001",
            "agent_role": "Developer",
            "instruction": "T001 Create test package directories `tests/unit/sql/`, `tests/unit/dbt/`, `tests/integration/` with `__init__.py` in each; confirm `tests/__init__.py` exists",
            "file_locks": [
              "__init__.py",
              "tests/__init__.py"
            ],
            "constitution_rules": [],
            "completion_handshake": "Upon success, update tasks.md line containing 'T001 Create test package directories `tests/unit/sql/`, `tests/unit/dbt/`, `tests/integration/` with `__init__.py` in each; confirm `tests/__init__.py` exists' to [x]",
            "dependencies": []
          },
          {
            "task_id": "T002",
            "agent_role": "Developer",
            "instruction": "T002 Create `cli/sentinel/sql_utils.py` with two exported functions: (1) `strip_jinja(sql: str) -> str` \u2014 replace `{{ ref('model') }}` \u2192 `model`, `{{ source('s','t') }}` \u2192 `s__t`, remaining `{{ ... }}` \u2192 `1`, `{% ... %}` \u2192 `` (empty) using `re.sub` with `re.DOTALL`; (2) `try_import_sqlglot()` \u2014 attempts `import sqlglot, sqlglot.expressions as exp`, returns `(sqlglot, exp)` tuple or `(None, None)` with `warnings.warn(\"sqlglot not installed; falling back to regex\")`",
            "file_locks": [
              "cli/sentinel/sql_utils.py",
              "warnings.warn",
              "re.DOTALL",
              "re.sub"
            ],
            "constitution_rules": [],
            "completion_handshake": "Upon success, update tasks.md line containing 'T002 Create `cli/sentinel/sql_utils.py` with two exported functions: (1) `strip_jinja(sql: str) -> str` \u2014 replace `{{ ref('model') }}` \u2192 `model`, `{{ source('s','t') }}` \u2192 `s__t`, remaining `{{ ... }}` \u2192 `1`, `{% ... %}` \u2192 `` (empty) using `re.sub` with `re.DOTALL`; (2) `try_import_sqlglot()` \u2014 attempts `import sqlglot, sqlglot.expressions as exp`, returns `(sqlglot, exp)` tuple or `(None, None)` with `warnings.warn(\"sqlglot not installed; falling back to regex\")`' to [x]",
            "dependencies": []
          },
          {
            "task_id": "T003",
            "agent_role": "Developer",
            "instruction": "T003 [US1] Create `cli/sentinel/sql_schema_diff.py` \u2014 implement `ColumnDef` dataclass (`name: str, data_type: str, nullable: bool, is_pk: bool`) and `DDLParser` class with `parse_ddl(sql: str) -> dict[str, list[ColumnDef]]` method: strip Jinja via `sql_utils.strip_jinja()`, try sqlglot `parse_one(sql)` + iterate `ast.find_all(exp.ColumnDef)` extracting name/type/nullable/pk; on ImportError or ParseError fall back to regex `r\"(\\w+)\\s+([\\w\\(\\),\\s]+?)(?:\\s+NOT NULL|\\s+NULL\\b|,|\\s*\\))\"`; return `{table_name: [ColumnDef, ...]}`",
            "file_locks": [
              "cli/sentinel/sql_schema_diff.py"
            ],
            "constitution_rules": [],
            "completion_handshake": "Upon success, update tasks.md line containing 'T003 [US1] Create `cli/sentinel/sql_schema_diff.py` \u2014 implement `ColumnDef` dataclass (`name: str, data_type: str, nullable: bool, is_pk: bool`) and `DDLParser` class with `parse_ddl(sql: str) -> dict[str, list[ColumnDef]]` method: strip Jinja via `sql_utils.strip_jinja()`, try sqlglot `parse_one(sql)` + iterate `ast.find_all(exp.ColumnDef)` extracting name/type/nullable/pk; on ImportError or ParseError fall back to regex `r\"(\\w+)\\s+([\\w\\(\\),\\s]+?)(?:\\s+NOT NULL|\\s+NULL\\b|,|\\s*\\))\"`; return `{table_name: [ColumnDef, ...]}`' to [x]",
            "dependencies": []
          },
          {
            "task_id": "T007",
            "agent_role": "Developer",
            "instruction": "T007 [US1] Modify `cli/sentinel/interface_diff.py`: locate the section that handles file extension routing; add an `elif path.suffix == \".sql\":` branch that imports `cli.sentinel.sql_schema_diff` and calls `DDLParser.parse_ddl(path.read_text())`, returning the column dict; this replaces the current Python-AST no-op for `.sql` files",
            "file_locks": [
              "cli/sentinel/interface_diff.py"
            ],
            "constitution_rules": [],
            "completion_handshake": "Upon success, update tasks.md line containing 'T007 [US1] Modify `cli/sentinel/interface_diff.py`: locate the section that handles file extension routing; add an `elif path.suffix == \".sql\":` branch that imports `cli.sentinel.sql_schema_diff` and calls `DDLParser.parse_ddl(path.read_text())`, returning the column dict; this replaces the current Python-AST no-op for `.sql` files' to [x]",
            "dependencies": []
          },
          {
            "task_id": "T008",
            "agent_role": "Developer",
            "instruction": "T008 [US1] Modify `cli/wave_executor.py`: (1) in `execute_wave()` before agent task dispatch, collect `sql_files = [t.file_path for t in wave.tasks if t.file_path and t.file_path.endswith(\".sql\")]`; if non-empty call `SchemaSnapshot.capture_pre_wave(wave.wave_id, sql_files)`; (2) in `verify_wave_completion()` call `report = SchemaDiff.compare_post_wave(wave.wave_id, sql_files)`; if `report.has_breaking_changes` raise `WaveCheckpointError` with formatted message listing each breaking change and its `affected_models`",
            "file_locks": [
              "cli/wave_executor.py"
            ],
            "constitution_rules": [],
            "completion_handshake": "Upon success, update tasks.md line containing 'T008 [US1] Modify `cli/wave_executor.py`: (1) in `execute_wave()` before agent task dispatch, collect `sql_files = [t.file_path for t in wave.tasks if t.file_path and t.file_path.endswith(\".sql\")]`; if non-empty call `SchemaSnapshot.capture_pre_wave(wave.wave_id, sql_files)`; (2) in `verify_wave_completion()` call `report = SchemaDiff.compare_post_wave(wave.wave_id, sql_files)`; if `report.has_breaking_changes` raise `WaveCheckpointError` with formatted message listing each breaking change and its `affected_models`' to [x]",
            "dependencies": []
          },
          {
            "task_id": "T009",
            "agent_role": "Developer",
            "instruction": "T009 [P] [US1] Write `tests/unit/sql/test_ddl_parser.py` \u2014 6 test functions: (1) parse `CREATE TABLE orders (id BIGINT NOT NULL PRIMARY KEY, total DECIMAL(10,2) NULL)` \u2192 column names `[\"id\",\"total\"]`, types correct, id.is_pk=True; (2) parse Jinja DDL with `{{ ref('x') }}` stripped correctly; (3) `nullable=True` for NULL columns, `nullable=False` for NOT NULL; (4) regex fallback path when sqlglot absent (monkeypatch `sql_utils.try_import_sqlglot` to return None,None); (5) unrecognised DDL \u2192 empty dict, no exception; (6) multiple tables in one file \u2192 dict with 2 keys",
            "file_locks": [
              "tests/unit/sql/test_ddl_parser.py"
            ],
            "constitution_rules": [],
            "completion_handshake": "Upon success, update tasks.md line containing 'T009 [P] [US1] Write `tests/unit/sql/test_ddl_parser.py` \u2014 6 test functions: (1) parse `CREATE TABLE orders (id BIGINT NOT NULL PRIMARY KEY, total DECIMAL(10,2) NULL)` \u2192 column names `[\"id\",\"total\"]`, types correct, id.is_pk=True; (2) parse Jinja DDL with `{{ ref('x') }}` stripped correctly; (3) `nullable=True` for NULL columns, `nullable=False` for NOT NULL; (4) regex fallback path when sqlglot absent (monkeypatch `sql_utils.try_import_sqlglot` to return None,None); (5) unrecognised DDL \u2192 empty dict, no exception; (6) multiple tables in one file \u2192 dict with 2 keys' to [x]",
            "dependencies": []
          },
          {
            "task_id": "T010",
            "agent_role": "Developer",
            "instruction": "T010 [P] [US1] Write `tests/unit/sql/test_schema_diff.py` \u2014 5 test functions using temp snapshot JSON files: (1) `COLUMN_REMOVED` \u2192 `is_breaking=True`, column in report `changes`; (2) `COLUMN_ADDED` \u2192 `is_breaking=False`; (3) `TYPE_CHANGED` BIGINT\u2192INT \u2192 `is_breaking=True`; (4) `TYPE_CHANGED` INT\u2192BIGINT \u2192 `is_breaking=False`; (5) no changes \u2192 `has_breaking_changes=False`, empty `changes` list",
            "file_locks": [
              "tests/unit/sql/test_schema_diff.py"
            ],
            "constitution_rules": [],
            "completion_handshake": "Upon success, update tasks.md line containing 'T010 [P] [US1] Write `tests/unit/sql/test_schema_diff.py` \u2014 5 test functions using temp snapshot JSON files: (1) `COLUMN_REMOVED` \u2192 `is_breaking=True`, column in report `changes`; (2) `COLUMN_ADDED` \u2192 `is_breaking=False`; (3) `TYPE_CHANGED` BIGINT\u2192INT \u2192 `is_breaking=True`; (4) `TYPE_CHANGED` INT\u2192BIGINT \u2192 `is_breaking=False`; (5) no changes \u2192 `has_breaking_changes=False`, empty `changes` list' to [x]",
            "dependencies": []
          },
          {
            "task_id": "T011",
            "agent_role": "Developer",
            "instruction": "T011 [US2] Create `cli/dbt_graph.py` \u2014 implement `DBTModel` dataclass (`name: str, file_path: str, materialization: str, has_description: bool, has_unique_key: bool, upstream: list[str], downstream: list[str], source: str = \"manifest\"`) and `DBTGraph` class with `load(project_root: str = \".\") -> \"DBTGraph\"`: if `{project_root}/target/manifest.json` exists, parse it \u2014 iterate `data[\"nodes\"]` for keys starting with `\"model.\"`, extract `name`, `config.materialized`, `depends_on.nodes` (filter to model-only deps), `description != \"\"`, presence of `unique_key` in config; else scan `{project_root}/models/**/*.sql` with `REF_PATTERN = re.compile(r'\\{\\{\\s*ref\\s*\\(\\s*[\\'\"](\\w+)[\\'\"]\\s*\\)\\s*\\}\\}')` and `CONFIG_PATTERN = re.compile(r\"materialized\\s*=\\s*['\\\"](\\w+)['\\\"]\")`; populate `self.nodes: dict[str, DBTModel]`; set downstream links by inverting upstream lists",
            "file_locks": [
              "{project_root}/target/manifest.json",
              "config.materialized",
              "target/manifest.json",
              "depends_on.nodes",
              "{project_root}/models/**/*.sql",
              "cli/dbt_graph.py"
            ],
            "constitution_rules": [],
            "completion_handshake": "Upon success, update tasks.md line containing 'T011 [US2] Create `cli/dbt_graph.py` \u2014 implement `DBTModel` dataclass (`name: str, file_path: str, materialization: str, has_description: bool, has_unique_key: bool, upstream: list[str], downstream: list[str], source: str = \"manifest\"`) and `DBTGraph` class with `load(project_root: str = \".\") -> \"DBTGraph\"`: if `{project_root}/target/manifest.json` exists, parse it \u2014 iterate `data[\"nodes\"]` for keys starting with `\"model.\"`, extract `name`, `config.materialized`, `depends_on.nodes` (filter to model-only deps), `description != \"\"`, presence of `unique_key` in config; else scan `{project_root}/models/**/*.sql` with `REF_PATTERN = re.compile(r'\\{\\{\\s*ref\\s*\\(\\s*[\\'\"](\\w+)[\\'\"]\\s*\\)\\s*\\}\\}')` and `CONFIG_PATTERN = re.compile(r\"materialized\\s*=\\s*['\\\"](\\w+)['\\\"]\")`; populate `self.nodes: dict[str, DBTModel]`; set downstream links by inverting upstream lists' to [x]",
            "dependencies": []
          },
          {
            "task_id": "T014",
            "agent_role": "Developer",
            "instruction": "T014 [US2] Modify `cli/orchestrator.py`: after standard file-lock wave grouping, add block: `if Path(\"dbt_project.yml\").exists():` \u2192 `graph = DBTGraph().load(\".\")` \u2192 `cycle = CycleDetector.detect_cycle(graph)` \u2192 if cycle `sys.exit(f\"ERROR: Circular dbt dependency: {cycle}\")` \u2192 `wave_overrides = DBTTopologicalSort.assign_waves(dbt_task_models, graph)` \u2192 remap affected task wave assignments in `execution_plan` using the override dict; tasks not in the dbt graph keep their file-lock-derived wave",
            "file_locks": [
              "cli/orchestrator.py",
              "sys.exit",
              "dbt_project.yml"
            ],
            "constitution_rules": [],
            "completion_handshake": "Upon success, update tasks.md line containing 'T014 [US2] Modify `cli/orchestrator.py`: after standard file-lock wave grouping, add block: `if Path(\"dbt_project.yml\").exists():` \u2192 `graph = DBTGraph().load(\".\")` \u2192 `cycle = CycleDetector.detect_cycle(graph)` \u2192 if cycle `sys.exit(f\"ERROR: Circular dbt dependency: {cycle}\")` \u2192 `wave_overrides = DBTTopologicalSort.assign_waves(dbt_task_models, graph)` \u2192 remap affected task wave assignments in `execution_plan` using the override dict; tasks not in the dbt graph keep their file-lock-derived wave' to [x]",
            "dependencies": []
          },
          {
            "task_id": "T015",
            "agent_role": "Developer",
            "instruction": "T015 [P] [US2] Write `tests/unit/dbt/test_dbt_graph.py` \u2014 5 test functions: (1) load from synthetic `manifest.json` fixture \u2192 correct `nodes` dict with names/materialization/upstream; (2) regex fallback scan synthetic `models/*.sql` files \u2192 detects `ref()` deps; (3) `has_description=True` when description is non-empty; (4) `has_unique_key=True` when `unique_key` present in config; (5) both paths produce equivalent graph for same project",
            "file_locks": [
              "tests/unit/dbt/test_dbt_graph.py",
              "manifest.json",
              "models/*.sql"
            ],
            "constitution_rules": [],
            "completion_handshake": "Upon success, update tasks.md line containing 'T015 [P] [US2] Write `tests/unit/dbt/test_dbt_graph.py` \u2014 5 test functions: (1) load from synthetic `manifest.json` fixture \u2192 correct `nodes` dict with names/materialization/upstream; (2) regex fallback scan synthetic `models/*.sql` files \u2192 detects `ref()` deps; (3) `has_description=True` when description is non-empty; (4) `has_unique_key=True` when `unique_key` present in config; (5) both paths produce equivalent graph for same project' to [x]",
            "dependencies": []
          },
          {
            "task_id": "T016",
            "agent_role": "Developer",
            "instruction": "T016 [P] [US2] Write `tests/unit/dbt/test_dbt_wave_planning.py` \u2014 4 test functions: (1) `stg_orders` + `dim_customers` (no upstream) \u2192 wave=1; `fct_orders` (refs both) \u2192 wave=2; (2) three independent models \u2192 all wave=1; (3) `CycleDetector` returns cycle path string for `a\u2192b\u2192a`; (4) `CycleDetector` returns None for valid DAG",
            "file_locks": [
              "tests/unit/dbt/test_dbt_wave_planning.py"
            ],
            "constitution_rules": [],
            "completion_handshake": "Upon success, update tasks.md line containing 'T016 [P] [US2] Write `tests/unit/dbt/test_dbt_wave_planning.py` \u2014 4 test functions: (1) `stg_orders` + `dim_customers` (no upstream) \u2192 wave=1; `fct_orders` (refs both) \u2192 wave=2; (2) three independent models \u2192 all wave=1; (3) `CycleDetector` returns cycle path string for `a\u2192b\u2192a`; (4) `CycleDetector` returns None for valid DAG' to [x]",
            "dependencies": []
          },
          {
            "task_id": "T017",
            "agent_role": "Developer",
            "instruction": "T017 [US3] Create `cli/sentinel/sql_constitution.py` \u2014 implement `ConstitutionViolation` dataclass (`rule: str, file_path: str, line: int, message: str`) and `SQLConstitutionScanner` class with `scan_file(path: str, enabled_rules: list[str]) -> list[ConstitutionViolation]`: read file, strip Jinja via `sql_utils.strip_jinja()`; for `NO_SELECT_STAR`: parse with sqlglot, detect `exp.Star` in any `exp.Select` (fallback regex `r\"SELECT\\s+\\*\\s+FROM\"`); for `NO_HARDCODED_CREDENTIALS`: apply `CREDENTIAL_PATTERNS = [re.compile(r\"(?i)(password|passwd|pwd)\\s*=\\s*'[^']{4,}'\"), re.compile(r\"(?i)(api_key|apikey|secret|token)\\s*=\\s*'[^']{8,}'\")]`; for `INCREMENTAL_NEEDS_UNIQUE_KEY`: check `materialized='incremental'` present without `unique_key=` in same config block; for `MIGRATION_NEEDS_ROLLBACK`: only for files in `migrations/` directory \u2014 check file contains at least one of `-- rollback`, `-- down`, `-- revert`, `-- undo` (case-insensitive); return violations list with correct line numbers",
            "file_locks": [
              "cli/sentinel/sql_constitution.py",
              "exp.Select",
              "exp.Star"
            ],
            "constitution_rules": [],
            "completion_handshake": "Upon success, update tasks.md line containing 'T017 [US3] Create `cli/sentinel/sql_constitution.py` \u2014 implement `ConstitutionViolation` dataclass (`rule: str, file_path: str, line: int, message: str`) and `SQLConstitutionScanner` class with `scan_file(path: str, enabled_rules: list[str]) -> list[ConstitutionViolation]`: read file, strip Jinja via `sql_utils.strip_jinja()`; for `NO_SELECT_STAR`: parse with sqlglot, detect `exp.Star` in any `exp.Select` (fallback regex `r\"SELECT\\s+\\*\\s+FROM\"`); for `NO_HARDCODED_CREDENTIALS`: apply `CREDENTIAL_PATTERNS = [re.compile(r\"(?i)(password|passwd|pwd)\\s*=\\s*'[^']{4,}'\"), re.compile(r\"(?i)(api_key|apikey|secret|token)\\s*=\\s*'[^']{8,}'\")]`; for `INCREMENTAL_NEEDS_UNIQUE_KEY`: check `materialized='incremental'` present without `unique_key=` in same config block; for `MIGRATION_NEEDS_ROLLBACK`: only for files in `migrations/` directory \u2014 check file contains at least one of `-- rollback`, `-- down`, `-- revert`, `-- undo` (case-insensitive); return violations list with correct line numbers' to [x]",
            "dependencies": []
          },
          {
            "task_id": "T019",
            "agent_role": "Developer",
            "instruction": "T019 [US3] Modify `cli/constitution_parser.py`: in the rule-extraction function, add SQL rule names to recognised rule set (`NO_SELECT_STAR`, `MODEL_DESCRIPTION_REQUIRED`, `INCREMENTAL_NEEDS_UNIQUE_KEY`, `NO_HARDCODED_CREDENTIALS`, `MIGRATION_NEEDS_ROLLBACK`); add `scan_sql_file(path, rules) -> list` and `scan_yaml_file(path, rules) -> list` routing functions that delegate to `SQLConstitutionScanner` and `DBTSchemaYAMLScanner` respectively",
            "file_locks": [
              "cli/constitution_parser.py"
            ],
            "constitution_rules": [],
            "completion_handshake": "Upon success, update tasks.md line containing 'T019 [US3] Modify `cli/constitution_parser.py`: in the rule-extraction function, add SQL rule names to recognised rule set (`NO_SELECT_STAR`, `MODEL_DESCRIPTION_REQUIRED`, `INCREMENTAL_NEEDS_UNIQUE_KEY`, `NO_HARDCODED_CREDENTIALS`, `MIGRATION_NEEDS_ROLLBACK`); add `scan_sql_file(path, rules) -> list` and `scan_yaml_file(path, rules) -> list` routing functions that delegate to `SQLConstitutionScanner` and `DBTSchemaYAMLScanner` respectively' to [x]",
            "dependencies": []
          },
          {
            "task_id": "T020",
            "agent_role": "Developer",
            "instruction": "T020 [US3] Modify `cli/sentinel/runner.py`: in the Tier 1 staged-file scan loop, add routing for `.sql` and `.yml` extensions \u2014 call `constitution_parser.scan_sql_file(path, active_rules)` for `.sql` files and `constitution_parser.scan_yaml_file(path, active_rules)` for `.yml` files; append violations to the existing violations list; format output as `\u274c VIOLATION [{rule}] {file}:{line}\\n   {message}`",
            "file_locks": [
              "cli/sentinel/runner.py"
            ],
            "constitution_rules": [],
            "completion_handshake": "Upon success, update tasks.md line containing 'T020 [US3] Modify `cli/sentinel/runner.py`: in the Tier 1 staged-file scan loop, add routing for `.sql` and `.yml` extensions \u2014 call `constitution_parser.scan_sql_file(path, active_rules)` for `.sql` files and `constitution_parser.scan_yaml_file(path, active_rules)` for `.yml` files; append violations to the existing violations list; format output as `\u274c VIOLATION [{rule}] {file}:{line}\\n   {message}`' to [x]",
            "dependencies": []
          },
          {
            "task_id": "T021",
            "agent_role": "Developer",
            "instruction": "T021 [P] [US3] Write `tests/unit/sql/test_sql_constitution.py` \u2014 10 test functions (2 per rule): `NO_SELECT_STAR` violation on `SELECT * FROM t`, pass on `SELECT id FROM t`; `MODEL_DESCRIPTION_REQUIRED` violation on YAML model with no `description:`, pass when description present; `INCREMENTAL_NEEDS_UNIQUE_KEY` violation on `config(materialized='incremental')` without `unique_key`, pass when `unique_key='id'`; `NO_HARDCODED_CREDENTIALS` violation on `PASSWORD = 'secret123'`, pass on parameterised query; `MIGRATION_NEEDS_ROLLBACK` violation on migration file with no rollback marker, pass when `-- rollback` present",
            "file_locks": [
              "tests/unit/sql/test_sql_constitution.py"
            ],
            "constitution_rules": [],
            "completion_handshake": "Upon success, update tasks.md line containing 'T021 [P] [US3] Write `tests/unit/sql/test_sql_constitution.py` \u2014 10 test functions (2 per rule): `NO_SELECT_STAR` violation on `SELECT * FROM t`, pass on `SELECT id FROM t`; `MODEL_DESCRIPTION_REQUIRED` violation on YAML model with no `description:`, pass when description present; `INCREMENTAL_NEEDS_UNIQUE_KEY` violation on `config(materialized='incremental')` without `unique_key`, pass when `unique_key='id'`; `NO_HARDCODED_CREDENTIALS` violation on `PASSWORD = 'secret123'`, pass on parameterised query; `MIGRATION_NEEDS_ROLLBACK` violation on migration file with no rollback marker, pass when `-- rollback` present' to [x]",
            "dependencies": []
          },
          {
            "task_id": "T022",
            "agent_role": "Developer",
            "instruction": "T022 [US4] Modify `cli/sentinel/placeholder_scanner.py`: add `SQL_PLACEHOLDER_PATTERNS = {\"SELECT_ONE\": re.compile(r\"^\\s*SELECT\\s+1\\s*(?:AS\\s+\\w+\\s*)?$\", re.MULTILINE | re.IGNORECASE), \"TODO_COMMENT\": re.compile(r\"--\\s*(TODO|FIXME|STUB|PLACEHOLDER)\\b\", re.IGNORECASE), \"EMPTY_REF\": re.compile(r'ref\\s*\\(\\s*[\\'\\\"]{2}\\s*\\)')}` and `scan_sql_file(path: str) -> list[PlaceholderViolation]` function: read file, iterate each pattern, for each match find line number via `content[:match.start()].count(\"\\n\") + 1`, return `PlaceholderViolation(file_path=path, line=line, pattern_type=pattern_name, matched_text=match.group(0).strip())`",
            "file_locks": [
              "cli/sentinel/placeholder_scanner.py"
            ],
            "constitution_rules": [],
            "completion_handshake": "Upon success, update tasks.md line containing 'T022 [US4] Modify `cli/sentinel/placeholder_scanner.py`: add `SQL_PLACEHOLDER_PATTERNS = {\"SELECT_ONE\": re.compile(r\"^\\s*SELECT\\s+1\\s*(?:AS\\s+\\w+\\s*)?$\", re.MULTILINE | re.IGNORECASE), \"TODO_COMMENT\": re.compile(r\"--\\s*(TODO|FIXME|STUB|PLACEHOLDER)\\b\", re.IGNORECASE), \"EMPTY_REF\": re.compile(r'ref\\s*\\(\\s*[\\'\\\"]{2}\\s*\\)')}` and `scan_sql_file(path: str) -> list[PlaceholderViolation]` function: read file, iterate each pattern, for each match find line number via `content[:match.start()].count(\"\\n\") + 1`, return `PlaceholderViolation(file_path=path, line=line, pattern_type=pattern_name, matched_text=match.group(0).strip())`' to [x]",
            "dependencies": []
          },
          {
            "task_id": "T025",
            "agent_role": "Developer",
            "instruction": "T025 [P] Validate all 5 quickstart.md scenarios manually (scenarios 1\u20135): run each bash block from `specs/001-sql-dbt-support/quickstart.md`, record actual vs expected output, update quickstart.md with actual output if they differ, mark each scenario PASS/FAIL",
            "file_locks": [
              "quickstart.md",
              "specs/001-sql-dbt-support/quickstart.md"
            ],
            "constitution_rules": [],
            "completion_handshake": "Upon success, update tasks.md line containing 'T025 [P] Validate all 5 quickstart.md scenarios manually (scenarios 1\u20135): run each bash block from `specs/001-sql-dbt-support/quickstart.md`, record actual vs expected output, update quickstart.md with actual output if they differ, mark each scenario PASS/FAIL' to [x]",
            "dependencies": []
          }
        ],
        "checkpoint_after": {
          "enabled": true,
          "verification_criteria": "Verify all Wave 1 tasks are marked [x] in tasks.md",
          "git_agent": "git-version-manager",
          "memory_bank_agent": "memory-bank-keeper"
        }
      },
      {
        "wave_id": 2,
        "strategy": "PARALLEL_SWARM",
        "rationale": "Wave 2: 5 independent task(s) with no file conflicts",
        "tasks": [
          {
            "task_id": "T004",
            "agent_role": "Developer",
            "instruction": "T004 [US1] Add `SchemaSnapshot` class to `cli/sentinel/sql_schema_diff.py` with `capture_pre_wave(wave_id: int, sql_files: list[str], snapshot_dir: str = \".claude/schema_snapshots\") -> None`: for each path in sql_files run `subprocess.run([\"git\", \"show\", f\"HEAD:{path}\"], ...)` (empty dict on error = new file); parse with `DDLParser.parse_ddl()`; serialise to `snapshot_dir/wave_{wave_id}_pre.json` as `{file_path: {table: [[col, type, nullable, pk], ...]}}`. Add `Path(snapshot_dir).mkdir(parents=True, exist_ok=True)` guard.",
            "file_locks": [
              "_pre.json",
              "subprocess.run",
              "cli/sentinel/sql_schema_diff.py",
              "snapshot_dir/wave_{wave_id}_pre.json"
            ],
            "constitution_rules": [],
            "completion_handshake": "Upon success, update tasks.md line containing 'T004 [US1] Add `SchemaSnapshot` class to `cli/sentinel/sql_schema_diff.py` with `capture_pre_wave(wave_id: int, sql_files: list[str], snapshot_dir: str = \".claude/schema_snapshots\") -> None`: for each path in sql_files run `subprocess.run([\"git\", \"show\", f\"HEAD:{path}\"], ...)` (empty dict on error = new file); parse with `DDLParser.parse_ddl()`; serialise to `snapshot_dir/wave_{wave_id}_pre.json` as `{file_path: {table: [[col, type, nullable, pk], ...]}}`. Add `Path(snapshot_dir).mkdir(parents=True, exist_ok=True)` guard.' to [x]",
            "dependencies": [
              "T003"
            ]
          },
          {
            "task_id": "T012",
            "agent_role": "Developer",
            "instruction": "T012 [US2] Add `DBTTopologicalSort` to `cli/dbt_graph.py` with `assign_waves(task_model_names: list[str], graph: DBTGraph) -> dict[str, int]`: build sub-graph containing only models in `task_model_names`; run iterative level-based topological sort (wave=1 for nodes with no in-scope upstreams, wave=max(upstream waves)+1 otherwise); return `{model_name: wave_number}`",
            "file_locks": [
              "cli/dbt_graph.py"
            ],
            "constitution_rules": [],
            "completion_handshake": "Upon success, update tasks.md line containing 'T012 [US2] Add `DBTTopologicalSort` to `cli/dbt_graph.py` with `assign_waves(task_model_names: list[str], graph: DBTGraph) -> dict[str, int]`: build sub-graph containing only models in `task_model_names`; run iterative level-based topological sort (wave=1 for nodes with no in-scope upstreams, wave=max(upstream waves)+1 otherwise); return `{model_name: wave_number}`' to [x]",
            "dependencies": [
              "T011"
            ]
          },
          {
            "task_id": "T018",
            "agent_role": "Developer",
            "instruction": "T018 [US3] Add `DBTSchemaYAMLScanner` class to `cli/sentinel/sql_constitution.py` with `scan_yaml(path: str, enabled_rules: list[str]) -> list[ConstitutionViolation]`: load YAML with `yaml.safe_load()` (fall back to line-by-line regex if PyYAML absent); for `MODEL_DESCRIPTION_REQUIRED`: iterate `data.get(\"models\", [])`, flag each model where `description` key absent or empty string; for `INCREMENTAL_NEEDS_UNIQUE_KEY`: check models where `config.materialized == \"incremental\"` and `config.unique_key` absent; include file path + line number (PyYAML provides line info via `Loader=yaml.Loader`/mark)",
            "file_locks": [
              "cli/sentinel/sql_constitution.py",
              "data.get",
              "Loader=yaml.Loader"
            ],
            "constitution_rules": [],
            "completion_handshake": "Upon success, update tasks.md line containing 'T018 [US3] Add `DBTSchemaYAMLScanner` class to `cli/sentinel/sql_constitution.py` with `scan_yaml(path: str, enabled_rules: list[str]) -> list[ConstitutionViolation]`: load YAML with `yaml.safe_load()` (fall back to line-by-line regex if PyYAML absent); for `MODEL_DESCRIPTION_REQUIRED`: iterate `data.get(\"models\", [])`, flag each model where `description` key absent or empty string; for `INCREMENTAL_NEEDS_UNIQUE_KEY`: check models where `config.materialized == \"incremental\"` and `config.unique_key` absent; include file path + line number (PyYAML provides line info via `Loader=yaml.Loader`/mark)' to [x]",
            "dependencies": [
              "T017"
            ]
          },
          {
            "task_id": "T023",
            "agent_role": "Developer",
            "instruction": "T023 [US4] Modify `cli/sentinel/runner.py` (Tier 1 scan section): for `.sql` staged files additionally call `placeholder_scanner.scan_sql_file(path)` and append results to violations list; format output as `\u274c PLACEHOLDER [{pattern_type}] {file}:{line}\\n   Found: {matched_text}`",
            "file_locks": [
              "cli/sentinel/runner.py"
            ],
            "constitution_rules": [],
            "completion_handshake": "Upon success, update tasks.md line containing 'T023 [US4] Modify `cli/sentinel/runner.py` (Tier 1 scan section): for `.sql` staged files additionally call `placeholder_scanner.scan_sql_file(path)` and append results to violations list; format output as `\u274c PLACEHOLDER [{pattern_type}] {file}:{line}\\n   Found: {matched_text}`' to [x]",
            "dependencies": [
              "T020"
            ]
          },
          {
            "task_id": "T024",
            "agent_role": "Developer",
            "instruction": "T024 [P] Write `tests/integration/test_sql_dbt_e2e.py` \u2014 one end-to-end test using `tempfile.mkdtemp()` + `git init`: (1) creates `migrations/001_orders.sql` with CREATE TABLE, commits; (2) removes a column (breaking change); (3) creates 3 dbt models (stg, dim, fct with ref()); (4) writes `dbt_project.yml`; (5) runs `orchestrator.py` as subprocess, verifies `execution_plan.json` has stg/dim in wave 1, fct in wave 2; (6) runs schema diff check, verifies `has_breaking_changes=True`; (7) adds `NO_SELECT_STAR` to `.constitution.md`, stages a `SELECT *` model, verifies violation caught; (8) verifies total runtime < 10 seconds",
            "file_locks": [
              "migrations/001_orders.sql",
              "constitution.md",
              "execution_plan.json",
              "dbt_project.yml",
              "tests/integration/test_sql_dbt_e2e.py",
              "orchestrator.py",
              ".constitution.md"
            ],
            "constitution_rules": [],
            "completion_handshake": "Upon success, update tasks.md line containing 'T024 [P] Write `tests/integration/test_sql_dbt_e2e.py` \u2014 one end-to-end test using `tempfile.mkdtemp()` + `git init`: (1) creates `migrations/001_orders.sql` with CREATE TABLE, commits; (2) removes a column (breaking change); (3) creates 3 dbt models (stg, dim, fct with ref()); (4) writes `dbt_project.yml`; (5) runs `orchestrator.py` as subprocess, verifies `execution_plan.json` has stg/dim in wave 1, fct in wave 2; (6) runs schema diff check, verifies `has_breaking_changes=True`; (7) adds `NO_SELECT_STAR` to `.constitution.md`, stages a `SELECT *` model, verifies violation caught; (8) verifies total runtime < 10 seconds' to [x]",
            "dependencies": [
              "T014"
            ]
          }
        ],
        "checkpoint_after": {
          "enabled": true,
          "verification_criteria": "Verify all Wave 2 tasks are marked [x] in tasks.md",
          "git_agent": "git-version-manager",
          "memory_bank_agent": "memory-bank-keeper"
        }
      },
      {
        "wave_id": 3,
        "strategy": "PARALLEL_SWARM",
        "rationale": "Wave 3: 2 independent task(s) with no file conflicts",
        "tasks": [
          {
            "task_id": "T005",
            "agent_role": "Developer",
            "instruction": "T005 [US1] Add `SchemaDiffReport` dataclass (`wave_id: int, has_breaking_changes: bool, changes: list[dict]`) and `SchemaDiff` class to `cli/sentinel/sql_schema_diff.py` with `compare_post_wave(wave_id: int, sql_files: list[str], snapshot_dir: str = \".claude/schema_snapshots\") -> SchemaDiffReport`: load pre-wave JSON; parse current file content with DDLParser; for each table compare column sets \u2014 `COLUMN_REMOVED` \u2192 `is_breaking=True`, `COLUMN_ADDED` \u2192 `is_breaking=False`, `TYPE_CHANGED` (narrower: INT\u2192SMALLINT, BIGINT\u2192INT, VARCHAR larger\u2192smaller) \u2192 `is_breaking=True`, widening \u2192 `is_breaking=False`; populate `affected_models` by scanning `models/**/*.sql` for files referencing both table name and column name",
            "file_locks": [
              "models/**/*.sql",
              "cli/sentinel/sql_schema_diff.py"
            ],
            "constitution_rules": [],
            "completion_handshake": "Upon success, update tasks.md line containing 'T005 [US1] Add `SchemaDiffReport` dataclass (`wave_id: int, has_breaking_changes: bool, changes: list[dict]`) and `SchemaDiff` class to `cli/sentinel/sql_schema_diff.py` with `compare_post_wave(wave_id: int, sql_files: list[str], snapshot_dir: str = \".claude/schema_snapshots\") -> SchemaDiffReport`: load pre-wave JSON; parse current file content with DDLParser; for each table compare column sets \u2014 `COLUMN_REMOVED` \u2192 `is_breaking=True`, `COLUMN_ADDED` \u2192 `is_breaking=False`, `TYPE_CHANGED` (narrower: INT\u2192SMALLINT, BIGINT\u2192INT, VARCHAR larger\u2192smaller) \u2192 `is_breaking=True`, widening \u2192 `is_breaking=False`; populate `affected_models` by scanning `models/**/*.sql` for files referencing both table name and column name' to [x]",
            "dependencies": [
              "T004",
              "T003"
            ]
          },
          {
            "task_id": "T013",
            "agent_role": "Developer",
            "instruction": "T013 [US2] Add `CycleDetector` to `cli/dbt_graph.py` with `detect_cycle(graph: DBTGraph) -> Optional[str]`: DFS with WHITE/GRAY/BLACK colouring; on back-edge found reconstruct cycle path as `\"a \u2192 b \u2192 c \u2192 a\"` string; return path string or `None` if DAG is acyclic",
            "file_locks": [
              "cli/dbt_graph.py"
            ],
            "constitution_rules": [],
            "completion_handshake": "Upon success, update tasks.md line containing 'T013 [US2] Add `CycleDetector` to `cli/dbt_graph.py` with `detect_cycle(graph: DBTGraph) -> Optional[str]`: DFS with WHITE/GRAY/BLACK colouring; on back-edge found reconstruct cycle path as `\"a \u2192 b \u2192 c \u2192 a\"` string; return path string or `None` if DAG is acyclic' to [x]",
            "dependencies": [
              "T012",
              "T011"
            ]
          }
        ],
        "checkpoint_after": {
          "enabled": true,
          "verification_criteria": "Verify all Wave 3 tasks are marked [x] in tasks.md",
          "git_agent": "git-version-manager",
          "memory_bank_agent": "memory-bank-keeper"
        }
      },
      {
        "wave_id": 4,
        "strategy": "SEQUENTIAL_MERGE",
        "rationale": "Wave 4: 1 independent task(s) with no file conflicts",
        "tasks": [
          {
            "task_id": "T006",
            "agent_role": "Developer",
            "instruction": "T006 [US1] Add `AffectedModelFinder` class to `cli/sentinel/sql_schema_diff.py` with `find_affected(table_name: str, column_name: str, models_dir: str = \"models\") -> list[str]`: walk `models/**/*.sql`, for each file check if it contains a ref/source to `table_name` AND the bare word `column_name` (word boundary `\\b` regex); return list of model base names (no path, no `.sql`)",
            "file_locks": [
              "models/**/*.sql",
              "cli/sentinel/sql_schema_diff.py"
            ],
            "constitution_rules": [],
            "completion_handshake": "Upon success, update tasks.md line containing 'T006 [US1] Add `AffectedModelFinder` class to `cli/sentinel/sql_schema_diff.py` with `find_affected(table_name: str, column_name: str, models_dir: str = \"models\") -> list[str]`: walk `models/**/*.sql`, for each file check if it contains a ref/source to `table_name` AND the bare word `column_name` (word boundary `\\b` regex); return list of model base names (no path, no `.sql`)' to [x]",
            "dependencies": [
              "T004",
              "T005",
              "T003"
            ]
          }
        ],
        "checkpoint_after": {
          "enabled": true,
          "verification_criteria": "Verify all Wave 4 tasks are marked [x] in tasks.md",
          "git_agent": "git-version-manager",
          "memory_bank_agent": "memory-bank-keeper"
        }
      }
    ]
  }
}