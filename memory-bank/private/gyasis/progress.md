# Progress

**Last Updated**: 2026-02-23 14:11:38

## Overall Progress
- Total Tasks: 25
- Completed: 25 ✅
- Pending: 0 ⏳
- Progress: 100%

## Task Breakdown
- [x] T001 Create test package directories `tests/unit/sql/`, `tests/unit/dbt/`, `tests/integration/` with `__init__.py` in each; confirm `tests/__init__.py` exists
- [x] T002 Create `cli/sentinel/sql_utils.py` with two exported functions: (1) `strip_jinja(sql: str) -> str` — replace `{{ ref('model') }}` → `model`, `{{ source('s','t') }}` → `s__t`, remaining `{{ ... }}` → `1`, `{% ... %}` → `` (empty) using `re.sub` with `re.DOTALL`; (2) `try_import_sqlglot()` — attempts `import sqlglot, sqlglot.expressions as exp`, returns `(sqlglot, exp)` tuple or `(None, None)` with `warnings.warn("sqlglot not installed; falling back to regex")`
- [x] T003 [US1] Create `cli/sentinel/sql_schema_diff.py` — implement `ColumnDef` dataclass (`name: str, data_type: str, nullable: bool, is_pk: bool`) and `DDLParser` class with `parse_ddl(sql: str) -> dict[str, list[ColumnDef]]` method: strip Jinja via `sql_utils.strip_jinja()`, try sqlglot `parse_one(sql)` + iterate `ast.find_all(exp.ColumnDef)` extracting name/type/nullable/pk; on ImportError or ParseError fall back to regex `r"(\w+)\s+([\w\(\),\s]+?)(?:\s+NOT NULL|\s+NULL\b|,|\s*\))"`; return `{table_name: [ColumnDef, ...]}`
- [x] T004 [US1] Add `SchemaSnapshot` class to `cli/sentinel/sql_schema_diff.py` with `capture_pre_wave(wave_id: int, sql_files: list[str], snapshot_dir: str = ".claude/schema_snapshots") -> None`: for each path in sql_files run `subprocess.run(["git", "show", f"HEAD:{path}"], ...)` (empty dict on error = new file); parse with `DDLParser.parse_ddl()`; serialise to `snapshot_dir/wave_{wave_id}_pre.json` as `{file_path: {table: [[col, type, nullable, pk], ...]}}`. Add `Path(snapshot_dir).mkdir(parents=True, exist_ok=True)` guard.
- [x] T005 [US1] Add `SchemaDiffReport` dataclass (`wave_id: int, has_breaking_changes: bool, changes: list[dict]`) and `SchemaDiff` class to `cli/sentinel/sql_schema_diff.py` with `compare_post_wave(wave_id: int, sql_files: list[str], snapshot_dir: str = ".claude/schema_snapshots") -> SchemaDiffReport`: load pre-wave JSON; parse current file content with DDLParser; for each table compare column sets — `COLUMN_REMOVED` → `is_breaking=True`, `COLUMN_ADDED` → `is_breaking=False`, `TYPE_CHANGED` (narrower: INT→SMALLINT, BIGINT→INT, VARCHAR larger→smaller) → `is_breaking=True`, widening → `is_breaking=False`; populate `affected_models` by scanning `models/**/*.sql` for files referencing both table name and column name
- [x] T006 [US1] Add `AffectedModelFinder` class to `cli/sentinel/sql_schema_diff.py` with `find_affected(table_name: str, column_name: str, models_dir: str = "models") -> list[str]`: walk `models/**/*.sql`, for each file check if it contains a ref/source to `table_name` AND the bare word `column_name` (word boundary `\b` regex); return list of model base names (no path, no `.sql`)
- [x] T007 [US1] Modify `cli/sentinel/interface_diff.py`: locate the section that handles file extension routing; add an `elif path.suffix == ".sql":` branch that imports `cli.sentinel.sql_schema_diff` and calls `DDLParser.parse_ddl(path.read_text())`, returning the column dict; this replaces the current Python-AST no-op for `.sql` files
- [x] T008 [US1] Modify `cli/wave_executor.py`: (1) in `execute_wave()` before agent task dispatch, collect `sql_files = [t.file_path for t in wave.tasks if t.file_path and t.file_path.endswith(".sql")]`; if non-empty call `SchemaSnapshot.capture_pre_wave(wave.wave_id, sql_files)`; (2) in `verify_wave_completion()` call `report = SchemaDiff.compare_post_wave(wave.wave_id, sql_files)`; if `report.has_breaking_changes` raise `WaveCheckpointError` with formatted message listing each breaking change and its `affected_models`
- [x] T009 [P] [US1] Write `tests/unit/sql/test_ddl_parser.py` — 6 test functions: (1) parse `CREATE TABLE orders (id BIGINT NOT NULL PRIMARY KEY, total DECIMAL(10,2) NULL)` → column names `["id","total"]`, types correct, id.is_pk=True; (2) parse Jinja DDL with `{{ ref('x') }}` stripped correctly; (3) `nullable=True` for NULL columns, `nullable=False` for NOT NULL; (4) regex fallback path when sqlglot absent (monkeypatch `sql_utils.try_import_sqlglot` to return None,None); (5) unrecognised DDL → empty dict, no exception; (6) multiple tables in one file → dict with 2 keys
- [x] T010 [P] [US1] Write `tests/unit/sql/test_schema_diff.py` — 5 test functions using temp snapshot JSON files: (1) `COLUMN_REMOVED` → `is_breaking=True`, column in report `changes`; (2) `COLUMN_ADDED` → `is_breaking=False`; (3) `TYPE_CHANGED` BIGINT→INT → `is_breaking=True`; (4) `TYPE_CHANGED` INT→BIGINT → `is_breaking=False`; (5) no changes → `has_breaking_changes=False`, empty `changes` list
- [x] T011 [US2] Create `cli/dbt_graph.py` — implement `DBTModel` dataclass (`name: str, file_path: str, materialization: str, has_description: bool, has_unique_key: bool, upstream: list[str], downstream: list[str], source: str = "manifest"`) and `DBTGraph` class with `load(project_root: str = ".") -> "DBTGraph"`: if `{project_root}/target/manifest.json` exists, parse it — iterate `data["nodes"]` for keys starting with `"model."`, extract `name`, `config.materialized`, `depends_on.nodes` (filter to model-only deps), `description != ""`, presence of `unique_key` in config; else scan `{project_root}/models/**/*.sql` with `REF_PATTERN = re.compile(r'\{\{\s*ref\s*\(\s*[\'"](\w+)[\'"]\s*\)\s*\}\}')` and `CONFIG_PATTERN = re.compile(r"materialized\s*=\s*['\"](\w+)['\"]")`; populate `self.nodes: dict[str, DBTModel]`; set downstream links by inverting upstream lists
- [x] T012 [US2] Add `DBTTopologicalSort` to `cli/dbt_graph.py` with `assign_waves(task_model_names: list[str], graph: DBTGraph) -> dict[str, int]`: build sub-graph containing only models in `task_model_names`; run iterative level-based topological sort (wave=1 for nodes with no in-scope upstreams, wave=max(upstream waves)+1 otherwise); return `{model_name: wave_number}`
- [x] T013 [US2] Add `CycleDetector` to `cli/dbt_graph.py` with `detect_cycle(graph: DBTGraph) -> Optional[str]`: DFS with WHITE/GRAY/BLACK colouring; on back-edge found reconstruct cycle path as `"a → b → c → a"` string; return path string or `None` if DAG is acyclic
- [x] T014 [US2] Modify `cli/orchestrator.py`: after standard file-lock wave grouping, add block: `if Path("dbt_project.yml").exists():` → `graph = DBTGraph().load(".")` → `cycle = CycleDetector.detect_cycle(graph)` → if cycle `sys.exit(f"ERROR: Circular dbt dependency: {cycle}")` → `wave_overrides = DBTTopologicalSort.assign_waves(dbt_task_models, graph)` → remap affected task wave assignments in `execution_plan` using the override dict; tasks not in the dbt graph keep their file-lock-derived wave
- [x] T015 [P] [US2] Write `tests/unit/dbt/test_dbt_graph.py` — 5 test functions: (1) load from synthetic `manifest.json` fixture → correct `nodes` dict with names/materialization/upstream; (2) regex fallback scan synthetic `models/*.sql` files → detects `ref()` deps; (3) `has_description=True` when description is non-empty; (4) `has_unique_key=True` when `unique_key` present in config; (5) both paths produce equivalent graph for same project
- [x] T016 [P] [US2] Write `tests/unit/dbt/test_dbt_wave_planning.py` — 4 test functions: (1) `stg_orders` + `dim_customers` (no upstream) → wave=1; `fct_orders` (refs both) → wave=2; (2) three independent models → all wave=1; (3) `CycleDetector` returns cycle path string for `a→b→a`; (4) `CycleDetector` returns None for valid DAG
- [x] T017 [US3] Create `cli/sentinel/sql_constitution.py` — implement `ConstitutionViolation` dataclass (`rule: str, file_path: str, line: int, message: str`) and `SQLConstitutionScanner` class with `scan_file(path: str, enabled_rules: list[str]) -> list[ConstitutionViolation]`: read file, strip Jinja via `sql_utils.strip_jinja()`; for `NO_SELECT_STAR`: parse with sqlglot, detect `exp.Star` in any `exp.Select` (fallback regex `r"SELECT\s+\*\s+FROM"`); for `NO_HARDCODED_CREDENTIALS`: apply `CREDENTIAL_PATTERNS = [re.compile(r"(?i)(password|passwd|pwd)\s*=\s*'[^']{4,}'"), re.compile(r"(?i)(api_key|apikey|secret|token)\s*=\s*'[^']{8,}'")]`; for `INCREMENTAL_NEEDS_UNIQUE_KEY`: check `materialized='incremental'` present without `unique_key=` in same config block; for `MIGRATION_NEEDS_ROLLBACK`: only for files in `migrations/` directory — check file contains at least one of `-- rollback`, `-- down`, `-- revert`, `-- undo` (case-insensitive); return violations list with correct line numbers
- [x] T018 [US3] Add `DBTSchemaYAMLScanner` class to `cli/sentinel/sql_constitution.py` with `scan_yaml(path: str, enabled_rules: list[str]) -> list[ConstitutionViolation]`: load YAML with `yaml.safe_load()` (fall back to line-by-line regex if PyYAML absent); for `MODEL_DESCRIPTION_REQUIRED`: iterate `data.get("models", [])`, flag each model where `description` key absent or empty string; for `INCREMENTAL_NEEDS_UNIQUE_KEY`: check models where `config.materialized == "incremental"` and `config.unique_key` absent; include file path + line number (PyYAML provides line info via `Loader=yaml.Loader`/mark)
- [x] T019 [US3] Modify `cli/constitution_parser.py`: in the rule-extraction function, add SQL rule names to recognised rule set (`NO_SELECT_STAR`, `MODEL_DESCRIPTION_REQUIRED`, `INCREMENTAL_NEEDS_UNIQUE_KEY`, `NO_HARDCODED_CREDENTIALS`, `MIGRATION_NEEDS_ROLLBACK`); add `scan_sql_file(path, rules) -> list` and `scan_yaml_file(path, rules) -> list` routing functions that delegate to `SQLConstitutionScanner` and `DBTSchemaYAMLScanner` respectively
- [x] T020 [US3] Modify `cli/sentinel/runner.py`: in the Tier 1 staged-file scan loop, add routing for `.sql` and `.yml` extensions — call `constitution_parser.scan_sql_file(path, active_rules)` for `.sql` files and `constitution_parser.scan_yaml_file(path, active_rules)` for `.yml` files; append violations to the existing violations list; format output as `❌ VIOLATION [{rule}] {file}:{line}\n   {message}`
- [x] T021 [P] [US3] Write `tests/unit/sql/test_sql_constitution.py` — 10 test functions (2 per rule): `NO_SELECT_STAR` violation on `SELECT * FROM t`, pass on `SELECT id FROM t`; `MODEL_DESCRIPTION_REQUIRED` violation on YAML model with no `description:`, pass when description present; `INCREMENTAL_NEEDS_UNIQUE_KEY` violation on `config(materialized='incremental')` without `unique_key`, pass when `unique_key='id'`; `NO_HARDCODED_CREDENTIALS` violation on `PASSWORD = 'secret123'`, pass on parameterised query; `MIGRATION_NEEDS_ROLLBACK` violation on migration file with no rollback marker, pass when `-- rollback` present
- [x] T022 [US4] Modify `cli/sentinel/placeholder_scanner.py`: add `SQL_PLACEHOLDER_PATTERNS = {"SELECT_ONE": re.compile(r"^\s*SELECT\s+1\s*(?:AS\s+\w+\s*)?$", re.MULTILINE | re.IGNORECASE), "TODO_COMMENT": re.compile(r"--\s*(TODO|FIXME|STUB|PLACEHOLDER)\b", re.IGNORECASE), "EMPTY_REF": re.compile(r'ref\s*\(\s*[\'\"]{2}\s*\)')}` and `scan_sql_file(path: str) -> list[PlaceholderViolation]` function: read file, iterate each pattern, for each match find line number via `content[:match.start()].count("\n") + 1`, return `PlaceholderViolation(file_path=path, line=line, pattern_type=pattern_name, matched_text=match.group(0).strip())`
- [x] T023 [US4] Modify `cli/sentinel/runner.py` (Tier 1 scan section): for `.sql` staged files additionally call `placeholder_scanner.scan_sql_file(path)` and append results to violations list; format output as `❌ PLACEHOLDER [{pattern_type}] {file}:{line}\n   Found: {matched_text}`
- [x] T024 [P] Write `tests/integration/test_sql_dbt_e2e.py` — one end-to-end test using `tempfile.mkdtemp()` + `git init`: (1) creates `migrations/001_orders.sql` with CREATE TABLE, commits; (2) removes a column (breaking change); (3) creates 3 dbt models (stg, dim, fct with ref()); (4) writes `dbt_project.yml`; (5) runs `orchestrator.py` as subprocess, verifies `execution_plan.json` has stg/dim in wave 1, fct in wave 2; (6) runs schema diff check, verifies `has_breaking_changes=True`; (7) adds `NO_SELECT_STAR` to `.constitution.md`, stages a `SELECT *` model, verifies violation caught; (8) verifies total runtime < 10 seconds
- [x] T025 [P] Validate all 5 quickstart.md scenarios manually (scenarios 1–5): run each bash block from `specs/001-sql-dbt-support/quickstart.md`, record actual vs expected output, update quickstart.md with actual output if they differ, mark each scenario PASS/FAIL

## Recent Milestones
c004bc3 [MILESTONE] Dev-kid initialized
